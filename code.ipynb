{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Data:\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"outage\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate samples for each class\n",
    "outage_samples = data[data['outage'] == 1]\n",
    "no_outage_samples = data[data['outage'] == 0]\n",
    "\n",
    "# Randomly sample 500 samples from each class for training data\n",
    "outage_training_samples = outage_samples.sample(n=500, random_state=42)\n",
    "no_outage_training_samples = no_outage_samples.sample(n=500, random_state=42)\n",
    "\n",
    "# Concatenate the samples from both classes for training data\n",
    "training_data = pd.concat([outage_training_samples, no_outage_training_samples])\n",
    "\n",
    "# Get the remaining samples for testing data\n",
    "outage_remaining_samples = outage_samples[~outage_samples.index.isin(outage_training_samples.index)]\n",
    "no_outage_remaining_samples = no_outage_samples[~no_outage_samples.index.isin(no_outage_training_samples.index)]\n",
    "\n",
    "# Randomly sample 2000 samples from each class for testing data\n",
    "outage_testing_samples = outage_remaining_samples.sample(n=2000, random_state=42)\n",
    "no_outage_testing_samples = no_outage_remaining_samples.sample(n=2000, random_state=42)\n",
    "\n",
    "# Concatenate the samples from both classes for testing data\n",
    "testing_data = pd.concat([outage_testing_samples, no_outage_testing_samples])\n",
    "\n",
    "# Verify the distribution\n",
    "print(training_data['outage'].value_counts())\n",
    "print(testing_data['outage'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length_train = training_data.groupby('outage')['text'].apply(lambda x: x.str.len().mean())\n",
    "avg_length_test = testing_data.groupby('outage')['text'].apply(lambda x: x.str.len().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_length_train)\n",
    "print(avg_length_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_length_test)\n",
    "print(avg_length_test.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def run_nlp_model(model_name, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Run NLP model workflow for XGBoost, SVM, or Logistic Regression.\n",
    "    \n",
    "    Args:\n",
    "    - model_name (str): Name of the model ('xgboost', 'svm', or 'logistic').\n",
    "    - train_data (DataFrame): Training data with 'text' and 'outage' columns.\n",
    "    - test_data (DataFrame): Testing data with 'text' and 'outage' columns.\n",
    "    \"\"\"\n",
    "    # Extract labels from the 'outage' column for training and testing data\n",
    "    train_labels = train_data['outage']\n",
    "    test_labels = test_data['outage']\n",
    "    \n",
    "    # Preprocessing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_features = vectorizer.fit_transform(train_data['text'])\n",
    "    test_features = vectorizer.transform(test_data['text'])\n",
    "    \n",
    "    # Model selection\n",
    "    if model_name == 'xgboost':\n",
    "        model = XGBClassifier()\n",
    "    elif model_name == 'svm':\n",
    "        model = SVC()\n",
    "    elif model_name == 'logistic':\n",
    "        model = LogisticRegression()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'xgboost', 'svm', or 'logistic'.\")\n",
    "    \n",
    "    # Training\n",
    "    model.fit(train_features, train_labels)\n",
    "    \n",
    "    # Testing\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(\"Model:\", model_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for SVM\n",
    "run_nlp_model('svm', training_data, testing_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for Logistic Regression\n",
    "run_nlp_model('logistic', training_data, testing_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for XGBoost\n",
    "run_nlp_model('xgboost', training_data, testing_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with LLMS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_zero_shot_classification(test_data, model_name):\n",
    "    # Initialize the zero-shot classification pipeline\n",
    "    if model_name == \"bert\":\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"bert-base-uncased\")\n",
    "    elif model_name == \"gpt\":\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"gpt2\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'bert' or 'gpt'.\")\n",
    "\n",
    "    # List of candidate labels\n",
    "    candidate_labels = [\"no_outage\", \"outage\"]\n",
    "\n",
    "    # Perform zero-shot classification on the testing data\n",
    "    results = classifier(\n",
    "        test_data['text'].tolist(),\n",
    "        candidate_labels,\n",
    "        multi_label=False\n",
    "    )\n",
    "\n",
    "    # Extract predicted labels and scores\n",
    "    predicted_labels = [result['labels'][0] for result in results]\n",
    "\n",
    "    # Convert true labels from 0/1 to 'no_outage'/'outage'\n",
    "    true_labels = test_data['outage'].map({0: 'no_outage', 1: 'outage'}).tolist()\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f\"{model_name.upper()} Zero-Shot Classification Metrics:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "evaluate_zero_shot_classification(testing_data, \"bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT\n",
    "evaluate_zero_shot_classification(testing_data, \"gpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_balanced_data(data, sample_size):\n",
    "    # Separate samples for each class\n",
    "    outage_samples = data[data['outage'] == 1]\n",
    "    no_outage_samples = data[data['outage'] == 0]\n",
    "\n",
    "    # Randomly sample an equal number of samples from each class\n",
    "    outage_samples = outage_samples.sample(n=sample_size, random_state=42)\n",
    "    no_outage_samples = no_outage_samples.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Concatenate the samples from both classes\n",
    "    balanced_data = pd.concat([outage_samples, no_outage_samples])\n",
    "\n",
    "    return balanced_data\n",
    "\n",
    "def run_llm_finetuning(training_data, testing_data, finetune_percentage, model_type):\n",
    "    # Create a balanced training dataset\n",
    "    num_samples = int(len(training_data) * finetune_percentage)\n",
    "    balanced_training_data = create_balanced_data(training_data, num_samples)\n",
    "\n",
    "    # Extract text and labels from the balanced training data\n",
    "    train_texts = balanced_training_data['text'].tolist()\n",
    "    train_labels = balanced_training_data['outage'].tolist()\n",
    "\n",
    "    # Extract text and labels from the testing data\n",
    "    test_texts = testing_data['text'].tolist()\n",
    "    test_labels = testing_data['outage'].tolist()\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=2)\n",
    "\n",
    "    # Tokenize the training and testing data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    # Define the dataset\n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    # Create the train and test datasets\n",
    "    train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "    test_dataset = CustomDataset(test_encodings, test_labels)\n",
    "\n",
    "    # Randomly sample 10% of the balanced training data for evaluation\n",
    "    eval_dataset_size = int(len(balanced_training_data) * 0.1)\n",
    "    eval_samples = balanced_training_data.sample(n=eval_dataset_size, random_state=42)\n",
    "    eval_texts = eval_samples['text'].tolist()\n",
    "    eval_labels = eval_samples['outage'].tolist()\n",
    "    eval_encodings = tokenizer(eval_texts, truncation=True, padding=True)\n",
    "    eval_dataset = CustomDataset(eval_encodings, eval_labels)\n",
    "\n",
    "    # Set up the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=10,\n",
    "        output_dir='./results',\n",
    "        # num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        save_strategy='epoch',\n",
    "        evaluation_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=lambda pred: {\"accuracy\": (pred.predictions.argmax(-1) == pred.label_ids).mean()},\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    precision = precision_score(test_labels, predicted_labels)\n",
    "    recall = recall_score(test_labels, predicted_labels)\n",
    "    f1 = f1_score(test_labels, predicted_labels)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "    print(f\"Finetuning Percentage: {finetune_percentage * 100}%\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "# Define the model types to evaluate\n",
    "model_types = ['bert-base-uncased', 'gpt2']\n",
    "\n",
    "# Run LLM finetuning and evaluation for different training percentages and model types\n",
    "train_percentages = [0.1, 0.2, 0.5, 0.75, 1]\n",
    "for percentage in train_percentages:\n",
    "    for model_type in model_types:\n",
    "        run_llm_finetuning(training_data, testing_data, percentage, model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
