{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Data:\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'id', 'likes', 'query', 'replies', 'retweets', 'text',\n",
       "       'user', 'outage', 'outage_state'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>query</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>outage</th>\n",
       "      <th>outage_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-01 23:50:22</td>\n",
       "      <td>264152432282578945</td>\n",
       "      <td>1</td>\n",
       "      <td>EversourceMA OR EversourceNH OR VelcoVT OR nat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Tom May, CEO of Northeast Utilities, the paren...</td>\n",
       "      <td>EversourceMA</td>\n",
       "      <td>1</td>\n",
       "      <td>WV OH PA NJ CT MA NY DE MD IN KY MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-01 23:45:13</td>\n",
       "      <td>264151136792109056</td>\n",
       "      <td>0</td>\n",
       "      <td>EversourceMA OR EversourceNH OR VelcoVT OR nat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>@NYGovCuomo @lipanews @nationalgridus @nyseand...</td>\n",
       "      <td>readyforthenet</td>\n",
       "      <td>1</td>\n",
       "      <td>WV OH PA NJ CT MA NY DE MD IN KY MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-01 23:34:44</td>\n",
       "      <td>264148498352590849</td>\n",
       "      <td>1</td>\n",
       "      <td>EversourceMA OR EversourceNH OR VelcoVT OR nat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Some amazing video from the Wareham microburst...</td>\n",
       "      <td>EversourceMA</td>\n",
       "      <td>1</td>\n",
       "      <td>WV OH PA NJ CT MA NY DE MD IN KY MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-01 23:34:20</td>\n",
       "      <td>264148399190851584</td>\n",
       "      <td>0</td>\n",
       "      <td>EversourceMA OR EversourceNH OR VelcoVT OR nat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>@nationalgridus Call me if you need some help ...</td>\n",
       "      <td>sparky1000</td>\n",
       "      <td>1</td>\n",
       "      <td>WV OH PA NJ CT MA NY DE MD IN KY MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-01 23:31:56</td>\n",
       "      <td>264147793147490304</td>\n",
       "      <td>0</td>\n",
       "      <td>EversourceMA OR EversourceNH OR VelcoVT OR nat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>Current PSNH statewide w/o power: 885. We're d...</td>\n",
       "      <td>EversourceNH</td>\n",
       "      <td>1</td>\n",
       "      <td>WV OH PA NJ CT MA NY DE MD IN KY MI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                  id  likes  \\\n",
       "0  2012-11-01 23:50:22  264152432282578945      1   \n",
       "1  2012-11-01 23:45:13  264151136792109056      0   \n",
       "2  2012-11-01 23:34:44  264148498352590849      1   \n",
       "3  2012-11-01 23:34:20  264148399190851584      0   \n",
       "4  2012-11-01 23:31:56  264147793147490304      0   \n",
       "\n",
       "                                               query  replies  retweets  \\\n",
       "0  EversourceMA OR EversourceNH OR VelcoVT OR nat...      1.0         3   \n",
       "1  EversourceMA OR EversourceNH OR VelcoVT OR nat...      0.0         0   \n",
       "2  EversourceMA OR EversourceNH OR VelcoVT OR nat...      0.0         1   \n",
       "3  EversourceMA OR EversourceNH OR VelcoVT OR nat...      0.0         0   \n",
       "4  EversourceMA OR EversourceNH OR VelcoVT OR nat...      1.0         8   \n",
       "\n",
       "                                                text            user  outage  \\\n",
       "0  Tom May, CEO of Northeast Utilities, the paren...    EversourceMA       1   \n",
       "1  @NYGovCuomo @lipanews @nationalgridus @nyseand...  readyforthenet       1   \n",
       "2  Some amazing video from the Wareham microburst...    EversourceMA       1   \n",
       "3  @nationalgridus Call me if you need some help ...      sparky1000       1   \n",
       "4  Current PSNH statewide w/o power: 885. We're d...    EversourceNH       1   \n",
       "\n",
       "                          outage_state  \n",
       "0  WV OH PA NJ CT MA NY DE MD IN KY MI  \n",
       "1  WV OH PA NJ CT MA NY DE MD IN KY MI  \n",
       "2  WV OH PA NJ CT MA NY DE MD IN KY MI  \n",
       "3  WV OH PA NJ CT MA NY DE MD IN KY MI  \n",
       "4  WV OH PA NJ CT MA NY DE MD IN KY MI  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38069"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outage\n",
       "1         20431\n",
       "0         17638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"outage\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    500\n",
      "1    500\n",
      "Name: outage, dtype: int64\n",
      "0    2000\n",
      "1    2000\n",
      "Name: outage, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate samples for each class\n",
    "outage_samples = data[data['outage'] == 1]\n",
    "no_outage_samples = data[data['outage'] == 0]\n",
    "\n",
    "# Randomly sample 500 samples from each class for training data\n",
    "outage_training_samples = outage_samples.sample(n=500, random_state=42)\n",
    "no_outage_training_samples = no_outage_samples.sample(n=500, random_state=42)\n",
    "\n",
    "# Concatenate the samples from both classes for training data\n",
    "training_data = pd.concat([outage_training_samples, no_outage_training_samples])\n",
    "\n",
    "# Get the remaining samples for testing data\n",
    "outage_remaining_samples = outage_samples[~outage_samples.index.isin(outage_training_samples.index)]\n",
    "no_outage_remaining_samples = no_outage_samples[~no_outage_samples.index.isin(no_outage_training_samples.index)]\n",
    "\n",
    "# Randomly sample 2000 samples from each class for testing data\n",
    "outage_testing_samples = outage_remaining_samples.sample(n=2000, random_state=42)\n",
    "no_outage_testing_samples = no_outage_remaining_samples.sample(n=2000, random_state=42)\n",
    "\n",
    "# Concatenate the samples from both classes for testing data\n",
    "testing_data = pd.concat([outage_testing_samples, no_outage_testing_samples])\n",
    "\n",
    "# Verify the distribution\n",
    "print(training_data['outage'].value_counts())\n",
    "print(testing_data['outage'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length_train = training_data.groupby('outage')['text'].apply(lambda x: x.str.len().mean())\n",
    "avg_length_test = testing_data.groupby('outage')['text'].apply(lambda x: x.str.len().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outage\n",
      "0    101.496\n",
      "1     99.430\n",
      "Name: text, dtype: float64\n",
      "100.463\n"
     ]
    }
   ],
   "source": [
    "print(avg_length_train)\n",
    "print(avg_length_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outage\n",
      "0    100.8235\n",
      "1     99.2205\n",
      "Name: text, dtype: float64\n",
      "100.02199999999999\n"
     ]
    }
   ],
   "source": [
    "print(avg_length_test)\n",
    "print(avg_length_test.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def run_nlp_model(model_name, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Run NLP model workflow for XGBoost, SVM, or Logistic Regression.\n",
    "    \n",
    "    Args:\n",
    "    - model_name (str): Name of the model ('xgboost', 'svm', or 'logistic').\n",
    "    - train_data (DataFrame): Training data with 'text' and 'outage' columns.\n",
    "    - test_data (DataFrame): Testing data with 'text' and 'outage' columns.\n",
    "    \"\"\"\n",
    "    # Extract labels from the 'outage' column for training and testing data\n",
    "    train_labels = train_data['outage']\n",
    "    test_labels = test_data['outage']\n",
    "    \n",
    "    # Preprocessing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_features = vectorizer.fit_transform(train_data['text'])\n",
    "    test_features = vectorizer.transform(test_data['text'])\n",
    "    \n",
    "    # Model selection\n",
    "    if model_name == 'xgboost':\n",
    "        model = XGBClassifier()\n",
    "    elif model_name == 'svm':\n",
    "        model = SVC()\n",
    "    elif model_name == 'logistic':\n",
    "        model = LogisticRegression()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'xgboost', 'svm', or 'logistic'.\")\n",
    "    \n",
    "    # Training\n",
    "    model.fit(train_features, train_labels)\n",
    "    \n",
    "    # Testing\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(\"Model:\", model_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: svm\n",
      "Accuracy: 0.6615\n",
      "Precision: 0.671443736730361\n",
      "Recall: 0.6325\n",
      "F1-score: 0.6513903192584963\n"
     ]
    }
   ],
   "source": [
    "# Call the function for SVM\n",
    "run_nlp_model('svm', training_data, testing_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic\n",
      "Accuracy: 0.6535\n",
      "Precision: 0.652281746031746\n",
      "Recall: 0.6575\n",
      "F1-score: 0.6548804780876494\n"
     ]
    }
   ],
   "source": [
    "# Call the function for Logistic Regression\n",
    "run_nlp_model('logistic', training_data, testing_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: xgboost\n",
      "Accuracy: 0.6015\n",
      "Precision: 0.5881841876629018\n",
      "Recall: 0.677\n",
      "F1-score: 0.6294746629474663\n"
     ]
    }
   ],
   "source": [
    "# Call the function for XGBoost\n",
    "run_nlp_model('xgboost', training_data, testing_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with LLMS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_zero_shot_classification(test_data, model_name):\n",
    "    # Initialize the zero-shot classification pipeline\n",
    "    if model_name == \"bert\":\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"bert-base-uncased\")\n",
    "    elif model_name == \"gpt\":\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"gpt2\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'bert' or 'gpt'.\")\n",
    "\n",
    "    # List of candidate labels\n",
    "    candidate_labels = [\"no_outage\", \"outage\"]\n",
    "\n",
    "    # Perform zero-shot classification on the testing data\n",
    "    results = classifier(\n",
    "        test_data['text'].tolist(),\n",
    "        candidate_labels,\n",
    "        multi_label=False\n",
    "    )\n",
    "\n",
    "    # Extract predicted labels and scores\n",
    "    predicted_labels = [result['labels'][0] for result in results]\n",
    "\n",
    "    # Convert true labels from 0/1 to 'no_outage'/'outage'\n",
    "    true_labels = test_data['outage'].map({0: 'no_outage', 1: 'outage'}).tolist()\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label=\"outage\")\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f\"{model_name.upper()} Zero-Shot Classification Metrics:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Zero-Shot Classification Metrics:\n",
      "Accuracy: 0.50025\n",
      "Precision: 0.5001610305958132\n",
      "Recall: 0.7765\n",
      "F1-score: 0.6084231145935356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "evaluate_zero_shot_classification(testing_data, \"bert\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151f0c64bc564219963d43bdc98420ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3099d69839394755bf39e74849602596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1524b68bc1ff423890e4c191dcc5367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c8d8cb1407479598e0fc41a1c5376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06bdad258424f718da29e57e1512aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Using pad_token, but it is not set yet.\n",
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Zero-Shot Classification Metrics:\n",
      "Accuracy: 0.49975\n",
      "Precision: 0.4998749061796347\n",
      "Recall: 0.999\n",
      "F1-score: 0.6663331665832917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_zero_shot_classification(testing_data, \"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
